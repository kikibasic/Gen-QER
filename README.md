# Gen-QERï¼ˆGenerative Query Expansion & Rerankingï¼‰: ç”Ÿæˆçš„ã‚¯ã‚¨ãƒªæ‹¡å¼µã¨ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°

[English](#english) | [æ—¥æœ¬èª](#japanese)

---

<a name="japanese"></a>
## ğŸ‡¯ğŸ‡µ æ—¥æœ¬èª

**Gen-QER**ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«(LLM)ã‚’ç”¨ã„ãŸã‚¯ã‚¨ãƒªæ‹¡å¼µã¨å¯†ãƒ™ã‚¯ãƒˆãƒ«ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã«ç„¦ç‚¹ã‚’å½“ã¦ãŸã€æƒ…å ±æ¤œç´¢(IR)ã‚¿ã‚¹ã‚¯ã®å®Ÿé¨“çš„å®Ÿè£…ã§ã™ã€‚

ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ä»¥ä¸‹ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æä¾›ã—ã¾ã™:
1. ã‚¹ãƒ‘ãƒ¼ã‚¹æ¤œç´¢(BM25)ã®å®Ÿè¡Œ
2. LLM(GPT-4oç­‰)ã‚’ç”¨ã„ãŸç–‘ä¼¼æ–‡æ›¸ã®ç”Ÿæˆ
3. **ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåŒ–ãƒ—ãƒ¼ãƒªãƒ³ã‚°**æˆ¦ç•¥ã‚’ç”¨ã„ãŸå¯†ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å™¨ã«ã‚ˆã‚‹ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°

---

### ğŸ› ï¸ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

**å¿…è¦ç’°å¢ƒ:** Python 3.10ä»¥ä¸Šã€Java 11ä»¥ä¸Š (Pyseriniã«å¿…è¦)

```bash
# 1. ä»®æƒ³ç’°å¢ƒã®ä½œæˆ
venv .venv
.venv/Scripts/Activate.ps1

# 2. Pythonä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt

# 3. BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
chmod +x scripts/download_data.sh
./scripts/download_data.sh
```

---

### ğŸƒâ€â™‚ï¸ ä½¿ç”¨æ–¹æ³•

GPTãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€OpenAI APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„:

```bash
export OPENAI_KEY="your-api-key-here"
```

ãƒ¡ã‚¤ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ:

```bash
# ä¾‹: DL19ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ
# - llm: ç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ« (gpt-4o, gpt-3.5-turbo, ã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹)
# - doc_gen: ç”Ÿæˆã™ã‚‹ç–‘ä¼¼æ–‡æ›¸ã®æ•° (ä¾‹: 2)
# - mode: ã‚¯ã‚¨ãƒªã¨æ–‡æ›¸ã‚’çµåˆã™ã‚‹æˆ¦ç•¥ ('contex-pool'ã‚’ä½¿ç”¨)
# - rank_model: å¯†ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å™¨ã®HuggingFaceãƒ¢ãƒ‡ãƒ«ID

python main.py \
  --irmode mugipipeline \
  --llm gpt-4o \
  --doc_gen 2 \
  --mode contex-pool \
  --rank_model BAAI/bge-large-en-v1.5
```

---

### ğŸ“‚ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

- **main.py**: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã€‚æ¤œç´¢ã‹ã‚‰è©•ä¾¡ã¾ã§ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å‡¦ç†
- **config.py**: ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã¨ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’å‡¦ç†
- **src/**: ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
  - **prompts.py**: LLMç”Ÿæˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å®šç¾©
  - **retriever.py**: å¯†ãƒ™ã‚¯ãƒˆãƒ«ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£… (ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåŒ–ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã‚’å«ã‚€)
  - **generator.py**: OpenAIã¨HuggingFaceãƒ¢ãƒ‡ãƒ«ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹
  - **searcher.py**: ã‚¹ãƒ‘ãƒ¼ã‚¹æ¤œç´¢ (Pyserini/BM25) ã¨ã‚¯ã‚¨ãƒªæ‹¡å¼µãƒ«ãƒ¼ãƒ—ã‚’å‡¦ç†
  - **evaluation.py**: trec_evalã®å®Ÿè¡Œã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
- **exp/**: ä¸­é–“çµæœã‚’ä¿å­˜ (æ¤œç´¢çµæœã¨ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€JSONãƒ•ã‚¡ã‚¤ãƒ«)
- **results/**: æœ€çµ‚è©•ä¾¡çµæœ (TRECå®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«) ã¨ã‚µãƒãƒªãƒ¼ãƒ­ã‚° (mugipipeline.json) ã‚’ä¿å­˜

---

### ğŸ“š å‚è€ƒæ–‡çŒ®

ã“ã®ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã¯MuGIã®å®Ÿè£…ã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚

```bibtex
@inproceedings{zhang-etal-2024-exploring-best,
    title = "Exploring the Best Practices of Query Expansion with Large Language Models",
    author = "Zhang, Le and Wu, Yihong and Yang, Qian and Nie, Jian-Yun",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    year = "2024"
}
```

---


<a name="english"></a>
## ğŸ‡¬ğŸ‡§ English

**Gen-QER** is an experimental implementation for Information Retrieval (IR) tasks, focusing on Query Expansion and Dense Reranking using Large Language Models (LLMs).

This repository provides a pipeline to:
1. Perform sparse retrieval (BM25).
2. Generate pseudo-documents using LLMs (GPT-4o, etc.).
3. Rerank search results using dense retrievers with **Contextualized Pooling** strategies.

---

### ğŸ› ï¸ Installation

**Requirements:** Python 3.10+, Java 11+ (Required for Pyserini)

```bash
# 1. Create a virtual environment (Conda is recommended)
conda create -n gen-qer python=3.10 openjdk=11 -c conda-forge -y
conda activate gen-qer

# 2. Install Python dependencies
pip install -r requirements.txt

# 3. Download BM25 indices and benchmark data
chmod +x scripts/download_data.sh
./scripts/download_data.sh
```

---

### ğŸƒâ€â™‚ï¸ Usage

Set your OpenAI API key if you intend to use GPT models:

```bash
export OPENAI_KEY="your-api-key-here"
```

Run the main pipeline:

```bash
# Example: Run pipeline on DL19 dataset
# - llm: Model for generation (gpt-4o, gpt-3.5-turbo, or local path)
# - doc_gen: Number of pseudo-documents to generate (e.g., 2)
# - mode: Strategy for combining query and documents (use 'contex-pool')
# - rank_model: HuggingFace model ID for the dense retriever

python main.py \
  --irmode mugipipeline \
  --llm gpt-4o \
  --doc_gen 2 \
  --mode contex-pool \
  --rank_model BAAI/bge-large-en-v1.5
```

---

### ğŸ“‚ Project Structure

- **main.py**: The main entry point for the pipeline. Handles the workflow from retrieval to evaluation.
- **config.py**: Handles command-line arguments and default configurations.
- **src/**: Source code modules.
  - **prompts.py**: Defines prompt templates for LLM generation.
  - **retriever.py**: Implements the dense reranking logic (including Contextualized Pooling).
  - **generator.py**: Wrapper class for OpenAI and HuggingFace models.
  - **searcher.py**: Handles sparse retrieval (Pyserini/BM25) and query expansion loops.
  - **evaluation.py**: Utilities for running trec_eval and calculating metrics.
- **exp/**: Stores intermediate results (JSON files containing search hits and generated text).
- **results/**: Stores final evaluation outputs (TREC run files) and summary logs (mugipipeline.json).

---

### ğŸ“š Reference

This codebase is based on the implementation of MuGI.

```bibtex
@inproceedings{zhang-etal-2024-exploring-best,
    title = "Exploring the Best Practices of Query Expansion with Large Language Models",
    author = "Zhang, Le and Wu, Yihong and Yang, Qian and Nie, Jian-Yun",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    year = "2024"
}
```
